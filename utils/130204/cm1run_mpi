#!/bin/csh
#
# Example of MPI run on NCAR's bluefire
#
# This example uses 32 processors (1 node) on bluefire.  The script requests
# 64 tasks/threads because bluefire uses Simultaneous Multi-Threading (SMT);
# see http://www.cisl.ucar.edu/docs/bluefire/be_quickstart.html#running
# for more information.  Always use SMT, because runs will finish ~20%
# faster.  Thus, always leave the "span[ptile=64]" line unmodified (i.e., 
# always request 64 tasks per node).
#
#   To use  64 processors (2 nodes), use #BSUB -n 128
#   To use 128 processors (4 nodes), use #BSUB -n 256
#   To use 256 processors (8 nodes), use #BSUB -n 512
#    ... etc ...
#
# Note:
# In namelist.input, nodex * nodey must equal the number entered in
# the BSUB -n line.  For the example below, I used nodex = 8 and nodey = 8.
#

#BSUB -a poe                            # use LSF poe elim
#BSUB -x                                # exclusive use of node
#BSUB -n 64                             # total tasks (MPI) needed
#BSUB -R "span[ptile=64]"               # max number of tasks (MPI) per node
#BSUB -J cm1run                         # job name
#BSUB -o cm1run.out                     # ouput filename
#BSUB -e cm1run.err                     # error filename
#BSUB -q regular                        # queue
#BSUB -W 6:00                           # wallclock limit (hh:ss)
#BSUB -P 64000461                       # project number

ldedit -btextpsize=64K -bdatapsize=64K -bstackpsize=64K cm1.exe
setenv TARGET_CPU_LIST "-1"
mpirun.lsf /usr/local/bin/launch ./cm1.exe >&! cm1.print.out

